# -*- coding: utf-8 -*-
"""breast cancer prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wsICl5SOjBRfLX_r8rkrFAFKtKtMOeo7
"""

import numpy as np 
import pandas as pd 

pd.options.display.max_columns = 100

import os
for dirname, _, filenames in os.walk('/kaggle/input/content/Breast_cancer_data.csv'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

data = pd.read_csv("/content/Breast_cancer_data.csv")

len(data.index), len(data.columns)

data.shape

data.head()
# daignosis here is telling us about being the diagnosed outcome, i.e if outcomes is 0 it represents it is non cancerous.

data.tail()

data.info()

data.isna()

data.isna().any()

data.isna().sum()

data = data.dropna(axis='columns')

print(type(data))

print(data.dtypes)

data.describe(include="int64")

data.diagnosis.value_counts()

data.head(2)

diagnosis_unique = data.diagnosis.unique()
diagnosis_unique

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# %matplotlib inline
sns.set_style('darkgrid')

print(type(data))

import seaborn as sns

sns.countplot(x='diagnosis', data=data)

px.histogram(data, x='diagnosis')

cols = ["diagnosis", "mean_radius", "mean_texture", "mean_perimeter", "mean_area","mean_smoothness"]

sns.pairplot(data[cols], hue="diagnosis")
plt.show()

size = len(data['mean_texture'])

area = np.pi * (15 * np.random.rand( size ))**2
colors = np.random.rand( size )

plt.xlabel("mean texture")
plt.ylabel("mean radius") 
plt.scatter(data['mean_texture'], data['mean_radius'], s=area, c=colors, alpha=0.5);

# here mean textture and radius means the nucleas of cancerous cells.

from sklearn.preprocessing import LabelEncoder

data.head(2)

labelencoder_Y = LabelEncoder()
data.diagnosis = labelencoder_Y.fit_transform(data.diagnosis)

data.head(2)

print(data.diagnosis.value_counts())
print("\n", data.diagnosis.value_counts().sum())

cols = ['diagnosis', 'mean_radius', 'mean_texture', 'mean_perimeter',
       'mean_area', 'mean_smoothness']
print(len(cols))
data[cols].corr()

plt.figure(figsize=(12, 9))

plt.title("Correlation Graph")

cmap = sns.diverging_palette( 1000, 120, as_cmap=True)
sns.heatmap(data[cols].corr(), annot=True, fmt='.1%',  linewidths=.05, cmap=cmap);

plt.figure(figsize=(15, 10))


fig = px.imshow(data[cols].corr());
fig.show()

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LogisticRegression

from sklearn.tree import DecisionTreeClassifier

from sklearn.ensemble import RandomForestClassifier

from sklearn.naive_bayes import GaussianNB

from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

from sklearn.metrics import classification_report

from sklearn.model_selection import KFold

from sklearn.model_selection import cross_validate, cross_val_score

from sklearn.svm import SVC

from sklearn import metrics

data.columns

prediction_feature = [ "mean_radius",  'mean_perimeter', 'mean_area', 'mean_smoothness', 'mean_texture']

targeted_feature = 'diagnosis'

len(prediction_feature)

X = data[prediction_feature]
X

y = data.diagnosis
y

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=15)

print(X_train)

# Scale the data to keep all the values in the same magnitude of 0 -1 

sc = StandardScaler()

X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

def model_building(model, X_train, X_test, y_train, y_test):
    """
    
    Model Fitting, Prediction And Other stuff
    return ('score', 'accuracy_score', 'predictions' )
    """
    
    model.fit(X_train, y_train)
    score = model.score(X_train, y_train)
    predictions = model.predict(X_test)
    accuracy = accuracy_score(predictions, y_test)
    
    return (score, accuracy, predictions)

models_list = {
    "LogisticRegression" :  LogisticRegression(),
    "RandomForestClassifier" :  RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=5),
    "DecisionTreeClassifier" :  DecisionTreeClassifier(criterion='entropy', random_state=0),
    "SVC" :  SVC(),
}

print(list(models_list.keys()))
print(list(models_list.values()))

# Let's Define the function for confision metric Graphs

def cm_metrix_graph(cm):
    
    sns.heatmap(cm,annot=True,fmt="d")
    plt.show()

df_prediction = []
confusion_matrixs = []
df_prediction_cols = [ 'model_name', 'score', 'accuracy_score' , "accuracy_percentage"]

for name, model in zip(list(models_list.keys()), list(models_list.values())):
    
    (score, accuracy, predictions) = model_building(model, X_train, X_test, y_train, y_test )
    
    print("\n\nClassification Report of '"+ str(name), "'\n")
    
    print(classification_report(y_test, predictions))

    df_prediction.append([name, score, accuracy, "{0:.2%}".format(accuracy)])
    
    # For Showing Metrics
    confusion_matrixs.append(confusion_matrix(y_test, predictions))
    
df_pred = pd.DataFrame(df_prediction, columns=df_prediction_cols)

df_pred = pd.DataFrame(df_prediction, columns=df_prediction_cols)

plt.figure(figsize=(10, 2))
# plt.title("Confusion Metric Graph")


for index, cm in enumerate(confusion_matrixs):
#     plt.xlabel("Negative Positive")
#     plt.ylabel("True Positive")

    
    
    # Show The Metrics Graph    
    cm_metrix_graph(cm) # Call the Confusion Metrics Graph
    plt.tight_layout(pad=True)

df_pred

df_pred.sort_values('score', ascending=False)
# df_pred.sort_values('accuracy_score', ascending=False)

len(data)
# print(len(X))

# Sample For testing only

cv_score = cross_validate(LogisticRegression(), X, y, cv=3,
                        scoring=('r2', 'neg_mean_squared_error'),
                        return_train_score=True)

pd.DataFrame(cv_score).describe().T

def cross_val_scorring(model):
    
#     (score, accuracy, predictions) = model_building(model, X_train, X_test, y_train, y_test )
    
    model.fit(data[prediction_feature], data[targeted_feature])
    
    # score = model.score(X_train, y_train)    
    
    predictions = model.predict(data[prediction_feature])    
    accuracy = accuracy_score(predictions, data[targeted_feature])
    print("\nFull-Data Accuracy:", round(accuracy, 2))
    print("Cross Validation Score of'"+ str(name), "'\n")
    
    
    # Initialize K folds.
    kFold = KFold(n_splits=5) # define 5 diffrent data folds
    
    err = []
    
    for train_index, test_index in kFold.split(data):
        # print("TRAIN:", train_index, "TEST:", test_index)

 # Data Spliting via fold indexes
        X_train = data[prediction_feature].iloc[train_index, :] # train_index = rows and all columns for Prediction_features
        y_train = data[targeted_feature].iloc[train_index] # all targeted features trains
        
        X_test = data[prediction_feature].iloc[test_index, :] # testing all rows and cols
        y_test = data[targeted_feature].iloc[test_index] # all targeted tests
        
        # Again Model Fitting
        model.fit(X_train, y_train)

        err.append(model.score(X_train, y_train))
        
        print("Score:", round(np.mean(err),  2) )

for name, model in zip(list(models_list.keys()), list(models_list.values())):
    cross_val_scorring(model)

from  sklearn.model_selection import GridSearchCV

# Let's Implement Grid Search Algorithm

# Pick the model

model = DecisionTreeClassifier()

# Tunning Params
param_grid = {'max_features': ['auto', 'sqrt', 'log2'],
              'min_samples_split': [2,3,4,5,6,7,8,9,10], 
              'min_samples_leaf':[2,3,4,5,6,7,8,9,10] }


# Implement GridSearchCV
gsc = GridSearchCV(model, param_grid, cv=10) # For 10 Cross-Validation

gsc.fit(X_train, y_train) # Model Fitting

print("\n Best Score is ")
print(gsc.best_score_)

print("\n Best Estinator is ")
print(gsc.best_estimator_)

print("\n Best Parametes are")
print(gsc.best_params_)

# Pick the model
model = KNeighborsClassifier()


# Tunning Params
param_grid = {
    'n_neighbors': list(range(1, 30)),
    'leaf_size': list(range(1,30)),
    'weights': [ 'distance', 'uniform' ]
}


# Implement GridSearchCV
gsc = GridSearchCV(model, param_grid, cv=10)

# Model Fitting
gsc.fit(X_train, y_train)

print("\n Best Score is ")
print(gsc.best_score_)

print("\n Best Estinator is ")
print(gsc.best_estimator_)

print("\n Best Parametes are")
print(gsc.best_params_)

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

# Define the grid of hyperparameters
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf']
}

# Create the grid search object with the desired estimator and hyperparameters
grid_search = GridSearchCV(SVC(), param_grid, cv=5)

# Fit the grid search to your data
grid_search.fit(X, y)

# Print the best estimator
print("\nBest Estimator:")
print(grid_search.best_estimator_)

# Print the best parameters
print("\nBest Parameters:")
print(grid_search.best_params_)

# Assuming you have imported the necessary libraries and defined the model
import pickle
from sklearn.linear_model import LogisticRegression

# Define and assign values to X_train and Y_train
X_train = [[1, 2], [3, 381]]  # Example input features
Y_train = [[381], [3]]  # Example output labels

# Initialize and fit the model with the training data
model = LogisticRegression()
model.fit(X_train, Y_train)

# Save the model to disk
filename = 'finalized_model.sav'
pickle.dump(model, open(filename, 'wb'))

model.fit(X_train, Y_train)
# save the model to disk
filename = 'finalized_model.sav'
pickle.dump(model, open(filename, 'wb'))

# Define and assign values to X_test and Y_test
X_test = [[7, 8], [9, 10]]  # Example test data features
Y_test = [[0], [1]]  # Example test data labels

# some time later...

# load the model from disk
loaded_model = pickle.load(open(filename, 'rb'))
result = loaded_model.score(X_test, Y_test)
print(result)

import pickle as pkl

import numpy as np

print("Shape of X_train:", np.shape(X_train))
print("Shape of y_train:", np.shape(y_train))

# Trainned Model # You can also use your own trainned model
logistic_model = LogisticRegression()

filename = 'logistic_model.pkl'
pkl.dump(logistic_model, open(filename, 'wb')) # wb means write as binary

# load the model from disk
loaded_model = pkl.load(open(filename, 'rb')) # rb means read as binary
result = loaded_model.score(X_test, Y_test)